input {
	log4j {
		host => "0.0.0.0"
		port => 4712 
#		codec => "json"
		mode => "server"
	}	

#	kafka {
#		bootstrap_servers => "128.55.12.59:9092"
#		topics => ["ta1-trace-1-e5-bgt-1"]
##		topics => ["ta1-fivedirections-1-e5-bgt-1"]
#		auto_offset_reset => "earliest"
#		key_deserializer_class => "org.apache.kafka.common.serialization.ByteArrayDeserializer"
#		value_deserializer_class => "org.apache.kafka.common.serialization.ByteArrayDeserializer"
##		key_deserializer_class => "com.bbn.tc.schema.serialization.kafka.KafkaAvroGenericDeserializer"
##		value_deserializer_class => "com.bbn.tc.schema.serialization.kafka.KafkaAvroGenericDeserializer"
#		group_id => "480bb378-4568-46af-bece-80257b0701e9"
##		group_id => "logstash"
#		codec => avro {
#			schema_uri => "/tmp/schema.avsc"
#		}
#	}
#	tcp {
#		port => 5000
#	}
}

## Add your filters / logstash plugins configuration here
filter {
    json {
        source => "message"
    }
    mutate {
       remove_field=>["message","timestamp","file","@version","path","thread","host","method","priority","logger_name","class"]
    }
    mutate {
        convert => {
            "[datum][com.bbn.tc.schema.avro.cdm20.Event][timestampNanos]" => "string"
        }
    }
    mutate {
        gsub => ["[datum][com.bbn.tc.schema.avro.cdm20.Event][timestampNanos]", "\d{6}$", ""]
    }
    date {
        match => ["[datum][com.bbn.tc.schema.avro.cdm20.Event][timestampNanos]", "UNIX_MS"]
        timezone => "America/New_York"
        locale => "en"  
        target => "@timestamp"
    }
}
output {
	#elasticsearch {
	#	hosts => "elasticsearch:9200"
	#	user => "elastic"
	#	password => "changeme"
	#}
	file {
		path => "/usr/share/logs/theia-%{+yyyy-MM-dd-HH}.json"
	}
	# stdout { codec => rubydebug }
}
